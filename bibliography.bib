@article{svm,
  title={Support-vector networks},
  author={Corinna Cortes and Vladimir Naumovich Vapnik},
  journal={Machine Learning},
  year={2004},
  volume={20},
  pages={273-297}
}

@thesis{alex,
title = {Identification of buildings from multimodal spatial data using machine learning techniques},
author = {Μάργαρης, Αλέξανδρος},
advisor = {Βασιλάς, Νικόλαος},
institution = {University of West Attica},
year = {2023},
month = {3},
doi = {http://dx.doi.org/10.26265/polynoe-3743},
}

@techreport{smo,
author = {Platt, John},
title = {Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines},
institution = {Microsoft},
year = {1998},
month = {4},
abstract = {This paper proposes a new algorithm for training support vector machines: Sequential Minimal Optimization, or SMO. Training a support vector machine requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while the standard chunking SVM algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. On real-world sparse data sets, SMO can be more than 1000 times faster than the chunking algorithm.},
url = {https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/},
number = {MSR-TR-98-14},
}
@inproceedings{csvm,
 author = {Graf, Hans and Cosatto, Eric and Bottou, L\'{e}on and Dourdanovic, Igor and Vapnik, Vladimir},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {L. Saul and Y. Weiss and L. Bottou},
 pages = {},
 publisher = {MIT Press},
 title = {Parallel Support Vector Machines: The Cascade SVM},
 url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/d756d3d2b9dac72449a6a6926534558a-Paper.pdf},
 volume = {17},
 year = {2004}
}

@article{treesvm,
title = {The one-against-all partition based binary tree support vector machine algorithms for multi-class classification},
journal = {Neurocomputing},
volume = {113},
pages = {1-7},
year = {2013},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2012.12.048},
url = {https://www.sciencedirect.com/science/article/pii/S0925231213002282},
author = {Xiaowei Yang and Qiaozhen Yu and Lifang He and Tengjiao Guo},
keywords = {Multi-class classification, SVM, OAA-SVM, R-OAA-SVM, Binary tree},
abstract = {The binary tree support vector machine (SVM) algorithm is one of the mainstream algorithms for multi-class classification in the fields of pattern recognition and machine learning. In order to reduce the training and testing time of one-against-all SVM (OAA-SVM) and reduced OAA-SVM (R-OAA-SVM), in this study, two OAA partition based binary tree SVM algorithms are proposed for multi-class classification. One is the single-space-mapped binary tree SVM (SBT-SVM) and the other is the multi-space-mapped binary tree SVM (MBT-SVM). In the proposed two algorithms, the best OAA partition is determined for each non-leaf node and the k-fold cross validation strategy is adopted to obtain the optimal classifiers. A set of experiments is conducted on nine UCI datasets and two face recognition datasets to demonstrate their performances. The results show that in term of testing accuracy, MBT-SVM is comparable with one-against-one SVM (OAO-SVM), R-OAA-SVM and OAA-SVM and superior to SBT-SVM. In term of testing time, MBT-SVM is superior to OAO-SVM, binary tree of SVM (BTS), R-OAA-SVM and OAA-SVM and slightly longer than SBT-SVM. In term of training time, MBT-SVM is superior to BTS, R-OAA-SVM and OAA-SVM and comparable with SBT-SVM. For the datasets with smaller class number and training sample number, the training time of MBT-SVM is comparable with that of OAO-SVM. For the datasets with larger class number or training sample number, in most cases, the training time of MBT-SVM is longer than that of OAO-SVM.}
}

@inproceedings{ova-ovo,
author = {Gupta, Surendra and Mehra, Neha},
year = {2013},
month = {07},
pages = {},
title = {Survey on Multiclass Classification Methods}
}

@article{fraud,
title = {The application of data mining techniques in financial fraud detection: A classification framework and an academic review of literature},
journal = {Decision Support Systems},
volume = {50},
number = {3},
pages = {559-569},
year = {2011},
note = {On quantitative methods for detection of financial fraud},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2010.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167923610001302},
author = {E.W.T. Ngai and Yong Hu and Y.H. Wong and Yijun Chen and Xin Sun},
keywords = {Financial fraud, Fraud detection, Literature review, Data mining, Business intelligence},
abstract = {This paper presents a review of — and classification scheme for — the literature on the application of data mining techniques for the detection of financial fraud. Although financial fraud detection (FFD) is an emerging topic of great importance, a comprehensive literature review of the subject has yet to be carried out. This paper thus represents the first systematic, identifiable and comprehensive academic literature review of the data mining techniques that have been applied to FFD. 49 journal articles on the subject published between 1997 and 2008 was analyzed and classified into four categories of financial fraud (bank fraud, insurance fraud, securities and commodities fraud, and other related financial fraud) and six classes of data mining techniques (classification, regression, clustering, prediction, outlier detection, and visualization). The findings of this review clearly show that data mining techniques have been applied most extensively to the detection of insurance fraud, although corporate fraud and credit card fraud have also attracted a great deal of attention in recent years. In contrast, we find a distinct lack of research on mortgage fraud, money laundering, and securities and commodities fraud. The main data mining techniques used for FFD are logistic models, neural networks, the Bayesian belief network, and decision trees, all of which provide primary solutions to the problems inherent in the detection and classification of fraudulent data. This paper also addresses the gaps between FFD and the needs of the industry to encourage additional research on neglected topics, and concludes with several suggestions for further FFD research.}
}
@article{medical,
title = {A machine learning based data modeling for medical diagnosis},
journal = {Biomedical Signal Processing and Control},
volume = {81},
pages = {104481},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104481},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422009351},
author = {Naeem Ahmed Mahoto and Asadullah Shaikh and Adel Sulaiman and Mana Saleh Al Reshan and Adel Rajab and Khairan Rajab},
keywords = {Machine learning, Medical data, Classification, Predictive models},
}

@Article{text,
AUTHOR = {Kowsari, Kamran and Jafari Meimandi, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
TITLE = {Text Classification Algorithms: A Survey},
JOURNAL = {Information},
VOLUME = {10},
YEAR = {2019},
NUMBER = {4},
ARTICLE-NUMBER = {150},
URL = {https://www.mdpi.com/2078-2489/10/4/150},
ISSN = {2078-2489},
ABSTRACT = {In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed.},
DOI = {10.3390/info10040150}
}


@article{unsupervised,
  title={Unsupervised learning},
  author={Dayan, Peter and Sahani, Maneesh and Deback, Gr{\'e}goire},
  journal={The MIT encyclopedia of the cognitive sciences},
  pages={857--859},
  year={1999},
  publisher={MIT Press}
}

@article{annotation,
author = {Suthaharan, Shan},
title = {Big Data Classification: Problems and Challenges in Network Intrusion Prediction with Machine Learning},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/2627534.2627557},
doi = {10.1145/2627534.2627557},
abstract = {This paper focuses on the specific problem of Big Data classification of network intrusion traffic. It discusses the system challenges presented by the Big Data problems associated with network intrusion prediction. The prediction of a possible intrusion attack in a network requires continuous collection of traffic data and learning of their characteristics on the fly. The continuous collection of traffic data by the network leads to Big Data problems that are caused by the volume, variety and velocity properties of Big Data. The learning of the network characteristics require machine learning techniques that capture global knowledge of the traffic patterns. The Big Data properties will lead to significant system challenges to implement machine learning frameworks. This paper discusses the problems and challenges in handling Big Data classification using geometric representation-learning techniques and the modern Big Data networking technologies. In particular this paper discusses the issues related to combining supervised learning techniques, representation-learning techniques, machine lifelong learning techniques and Big Data technologies (e.g. Hadoop, Hive and Cloud) for solving network traffic classification problems.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = {4},
pages = {70–73},
numpages = {4},
keywords = {intrusion detection, machine learning, hadoop distributed file systems, big data}
}

@Inbook{supervised,
author="Cunningham, P{\'a}draig
and Cord, Matthieu
and Delany, Sarah Jane",
editor="Cord, Matthieu
and Cunningham, P{\'a}draig",
title="Supervised Learning",
bookTitle="Machine Learning Techniques for Multimedia: Case Studies on Organization and Retrieval",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="21--49",
abstract="Supervised learning accounts for a lot of research activity in machine learning and many supervised learning techniques have found application in the processing of multimedia content. The defining characteristic of supervised learning is the availability of annotated training data. The name invokes the idea of a `supervisor' that instructs the learning system on the labels to associate with training examples. Typically these labels are class labels in classification problems. Supervised learning algorithms induce models from these training data and these models can be used to classify other unlabelled data. In this chapter we ground or analysis of supervised learning on the theory of risk minimization. We provide an overview of support vector machines and nearest neighbour classifiers{\textasciitilde}-- probably the two most popular supervised learning techniques employed in multimedia research.",
isbn="978-3-540-75171-7",
doi="10.1007/978-3-540-75171-7_2",
url="https://doi.org/10.1007/978-3-540-75171-7_2"
}

@article{online,
abstract = {Online learning represents a family of machine learning methods, where a learner attempts to tackle some predictive (or any type of decision-making) task by learning from a sequence of data instances one by one at each time. The goal of online learning is to maximize the accuracy/correctness for the sequence of predictions/decisions made by the online learner given the knowledge of correct answers to previous prediction/learning tasks and possibly additional information. This is in contrast to traditional batch or offline machine learning methods that are often designed to learn a model from the entire training data set at once. Online learning has become a promising technique for learning from continuous streams of data in many real-world applications. This survey aims to provide a comprehensive survey of the online machine learning literature through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques. Generally speaking, according to the types of learning tasks and the forms of feedback information, the existing online learning works can be classified into three major categories: (i) online supervised learning where full feedback information is always available, (ii) online learning with limited feedback, and (iii) online unsupervised learning where no feedback is available. Due to space limitation, the survey will be mainly focused on the first category, but also briefly cover some basics of the other two categories. Finally, we also discuss some open issues and attempt to shed light on potential future research directions in this field.},
author = {Steven C.H. Hoi and Doyen Sahoo and Jing Lu and Peilin Zhao},
doi = {https://doi.org/10.1016/j.neucom.2021.04.112},
issn = {0925-2312},
journal = {Neurocomputing},
keywords = {Online learning, Online convex optimization, Sequential decision making},
pages = {249-289},
title = {Online learning: A comprehensive survey},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221006706},
volume = {459},
year = {2021},
}
@incollection{online-realtime,
	doi = {10.1007/978-3-319-31750-2_6},
	url = {https://doi.org/10.1007%2F978-3-319-31750-2_6},
	year = 2016,
	publisher = {Springer International Publishing},
	pages = {67--78},
	author = {Biwei Liang and Tengjiao Wang and Shun Li and Wei Chen and Hongyan Li and Kai Lei},
	title = {Online Learning for Accurate Real-Time Map Matching},
	booktitle = {Advances in Knowledge Discovery and Data Mining}
}

@book{neural-networks,
  title={An introduction to neural networks},
  author={Gurney, Kevin},
  year={1997},
  publisher={CRC press}
}

@book{deep-learning,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@INPROCEEDINGS{cnn,
  author={Ajit, Arohan and Acharya, Koustav and Samanta, Abhishek},
  booktitle={2020 International Conference on Emerging Trends in Information Technology and Engineering (ic-ETITE)}, 
  title={A Review of Convolutional Neural Networks}, 
  year={2020},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ic-ETITE47903.2020.049}}


@slides{online-slides,
    author={Yingyu Liang},
    title={COS 495: Machine Learning Basics Lecture 3: Perceptron},
    url={https://www.cs.princeton.edu/courses/archive/spring16/cos495/slides/ML_basics_lecture3_Perceptron.pdf},
    notes={Online: accesed 2023-07-05},
    organization={Princeton University}
}

@article{rosenbaltt1957perceptron,
  title={The perceptron--a perciving and recognizing automation},
  author={Rosenbaltt, Frank},
  journal={Cornell Aeronautical Laboratory},
  year={1957}
}

@article{minsky1969introduction,
  title={An introduction to computational geometry},
  author={Minsky, Marvin and Papert, Seymour},
  journal={Cambridge tiass., HIT},
  volume={479},
  number={480},
  pages={104},
  year={1969}
}
@article{mcculloch43a,
  added-at = {2008-02-26T11:58:58.000+0100},
  author = {Mcculloch, Warren and Pitts, Walter},
  biburl = {https://www.bibsonomy.org/bibtex/26fbacb0ae04bc17d296d9265dfc90dff/schaul},
  citeulike-article-id = {2380493},
  description = {idsia},
  interhash = {3e8e0d06f376f3eb95af89d5a2f15957},
  intrahash = {6fbacb0ae04bc17d296d9265dfc90dff},
  journal = {Bulletin of Mathematical Biophysics},
  keywords = {evolutionary},
  pages = {127--147},
  priority = {2},
  timestamp = {2008-02-26T12:00:58.000+0100},
  title = {A Logical Calculus of Ideas Immanent in Nervous Activity},
  volume = 5,
  year = 1943
}

@book{hastie2001elements,
  title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author={Hastie, T. and Tibshirani, R. and Friedman, J.H.},
  isbn={9780387952840},
  lccn={20031433},
  series={Springer series in statistics},
  url={https://books.google.gr/books?id=VRzITwgNV2UC},
  year={2001},
  publisher={Springer},
  pages = {349--350}
}


@misc{schmidhuber2022annotated,
      title={Annotated History of Modern AI and Deep Learning}, 
      author={Juergen Schmidhuber},
      year={2022},
      eprint={2212.11279},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      pages = {9--10}
}

@inproceedings{macqueen1967some,
  title={Some methods for classification and analysis of multivariate observations},
  author={MacQueen, James and others},
  booktitle={Proceedings of the fifth Berkeley symposium on mathematical statistics and probability},
  volume={1},
  number={14},
  pages={281--297},
  year={1967},
  organization={Oakland, CA, USA}
}

@article{fix1989discriminatory,
  title={Discriminatory analysis. Nonparametric discrimination: Consistency properties},
  author={Fix, Evelyn and Hodges, Joseph Lawson},
  journal={International Statistical Review/Revue Internationale de Statistique},
  volume={57},
  number={3},
  pages={238--247},
  year={1989},
  publisher={JSTOR}
}

@ARTICLE{onenearest,
  author={Cover, T. and Hart, P.},
  journal={IEEE Transactions on Information Theory}, 
  title={Nearest neighbor pattern classification}, 
  year={1967},
  volume={13},
  number={1},
  pages={21-27},
  doi={10.1109/TIT.1967.1053964}}


@ARTICLE{Levenshtein,
       author = {{Levenshtein}, V.~I.},
        title = "{Binary Codes Capable of Correcting Deletions, Insertions and Reversals}",
      journal = {Soviet Physics Doklady},
         year = 1966,
        month = feb,
       volume = {10},
        pages = {707},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1966SPhD...10..707L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{kdtree,
author = {Bentley, Jon Louis},
title = {Multidimensional Binary Search Trees Used for Associative Searching},
year = {1975},
issue_date = {Sept. 1975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/361002.361007},
doi = {10.1145/361002.361007},
abstract = {This paper develops the multidimensional binary search tree (or k-d tree, where k is the dimensionality of the search space) as a data structure for storage of information to be retrieved by associative searches. The k-d tree is defined and examples are given. It is shown to be quite efficient in its storage requirements. A significant advantage of this structure is that a single data structure can handle many types of queries very efficiently. Various utility algorithms are developed; their proven average running times in an n record file are: insertion, O(log n); deletion of the root, O(n(k-1)/k); deletion of a random node, O(log n); and optimization (guarantees logarithmic performance of searches), O(n log n). Search algorithms are given for partial match queries with t keys specified [proven maximum running time of O(n(k-t)/k)] and for nearest neighbor queries [empirically observed average running time of O(log n).] These performances far surpass the best currently known algorithms for these tasks. An algorithm is presented to handle any general intersection query. The main focus of this paper is theoretical. It is felt, however, that k-d trees could be quite useful in many applications, and examples of potential uses are given.},
journal = {Commun. ACM},
month = {sep},
pages = {509–517},
numpages = {9},
keywords = {partial match queries, attribute, binary search trees, key, information retrieval system, binary tree insertion, nearest neighbor queries, associative retrieval, intersection queries}
}

@book{omohundro1989five,
  title={Five balltree construction algorithms},
  author={Omohundro, Stephen M},
  year={1989},
  publisher={International Computer Science Institute Berkeley}
}

@article{zhang2004optimality,
  title={The optimality of naive Bayes},
  author={Zhang, Harry},
  journal={Aa},
  volume={1},
  number={2},
  pages={3},
  year={2004}
}

@inproceedings{naivebayes,
author = {Metsis, Vangelis and Androutsopoulos, Ion and Paliouras, Georgios},
year = {2006},
month = {01},
pages = {},
title = {Spam Filtering with Naive Bayes - Which Naive Bayes?},
journal = {In CEAS}
}

@misc{kkt,
author = {Geoff Gordon and Ryan Tibshirani},
title = {Karush-Kuhn-Tucker conditions, Optimization},
url = {http://www.cs.cmu.edu/~ggordon/10725-F12/slides/16-kkt.pdf}
}
@book{precog,
author = {Theodoridis, S. and Koutroumbas, Konstantinos},
year = {2003},
month = {01},
pages = {},
title = {Pattern Recognition},
isbn = {0-12-685875-6}
}
