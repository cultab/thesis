
# Classification {#sec:class}

Classification is the process of assigning a class label to each sample of a dataset, an algorithm that classifies data is called a Classifier. It has a wide range of applications ranging from medical diagnosis [@medical] to natural language processing [@text] and building recognition [@alex] to financial fraud detection [@fraud]. As such lots of research has gone into improving classification accuracy (as well as minimizing run time)(?). This section will cover the training of classifiers, various categories of classifiers and finally various types of classifiers used today.

## Learning Strategies(?)

Classification is usually(?) split into two distinct phases, the training phase: wherein the classifier learns how to classify samples, and the prediction phase: wherein the classifier actually attempts to classify a dataset. But depending on when the training phase happens there exist multiple training strategies.

### Batch Learning {#sec:batch}

In batch training, the training and prediction phases **are** distinct and the training happens all at once--- before the prediction stage.

#### Training Phase

The first phase is the training phase wherein the classifier will be given a dataset that is representative of the data it's meant to later classify.

##### Supervised learning

The datasets used in training are usually annotated with the class label of each data point.
In supervised learning [@supervised], the dataset used in training will be annotated with the class label of each sample. In this way the classifier can learn to extract sets of features that are representative of each class by summarizing(?) the common features of samples with the same class label.

##### Unsupervised learning

Techniques that can operate on unlabeled datasets exist and fall under the label of unsupervised learning [@unsupervised].
In unsupervised learning, the dataset used in training is not annotated. Unsupervised learning techniques are used when a training dataset does not exist or is prohibitively expensive, time consuming or inconvenient to annotate correctly [@annotation].

#### Prediction Phase

The prediction phase is what happens after successful training of a classifier. The classifier is given a number of samples and assigns each a class label or a probability for each class label.

### Online learning

With online learning, as opposed to \nameref{sec:batch}, the classifier is designed to learn on $\ell + 1$ samples, where $\ell$ is the number of samples already having been used for training. In other words the classifier is given an unlabeled sample, it makes a prediction, and then the classifier is given the correct label which it uses to update it's model [@online-slides, page 20]. Online learning can be used in conjunction with either unsupervised and supervised learning. Online learning is useful when the training data is prohibitively large [@online], is generated in real time [@online-realtime] or when training has to be interleaved with prediction.

## Binary and Multiclass {#sec:bmclass}

Depending on the number of classes in a dataset there exist two kinds of classification. Binary classification is when the number of classes is exactly two and multiclass classification is for more than two classes. Not all kinds of classifiers do inherently support multiclass classification, but one can implement multiclass classifiers by using multiple binary classifiers. There are two main methods of extending binary classifiers to be used in multiclass classification.

### One vs All

One vs All^[One vs All: Also known as One vs Rest (OVR), One Against All (OAA) or One Against Rest (OAR)] (OVA) [@ova-ovo] multiclass classifiers have a classifier for each class. For $n$ classes, $n$ binary classifiers are required, one for each class. Each binary classifier distinguishes whether a sample belongs to a specific class or not (or any of the rest)(?). To assign the final class label to the sample the class label with the highest prediction confidence is used.

### One vs One

One vs One^[One vs one: Also known as One Against One (OAO)] (OVO) [@ova-ovo] multiclass classifiers have a classifier for every 2-combination^[2-combination: a distinct selection of 2 elements from a set] of classes, for $n$ classes $n \choose 2$ binary classifiers are required. 

\begin{equation}
\binom{n}{2} = \frac{n (n - 2 + 1)}{2 (2 - 1) 1} = \frac{n(n-1)}{2}
\end{equation}

Each binary classifier labels whether a sample belongs to one of it's two classes classes. The final class label is assigned to a sample using the class label predicted by the majority of the binary classifiers.

## Linear Classifiers {#sec:linear}

A linear classifier is a classifier that can only correctly predict the class of all it's inputs correctly if the dataset is linearly separable.
For a dataset to be linearly separable there must exist at least one hyperplane^[a subset with dimension $n-1$ where $n$ is the dimension of the dataset] that can create two half-spaces where each one only contains one class of samples(?).

more abstractly a set $X$ with distinct subsets $X_0$ and $X_1$ is said to be linearly separable if there exist $n+1$, $w_1, w_2, \unicodeellipsis, w_n, k$ where $w_i, k \in \BbbR$ satisfying

\begin{align}
\sum^{n}_{i=1} w_i x_i > k & \\
\sum^{n}_{i=1} w_i x_i < k &
\end{align}

where $x_i$ is the $i\textrm{-th}$ element of $X_0$ and $X_1$ respectively.

Non linear classifiers do not have this limitation.

## Classifiers

This subsection will go over a number of classifiers, as well as (their way of operation)(?). It will also go over the basics of SVM, but more details will be given in \nameref{sec:SVM}

### Perceptron

The perceptron was first described as the McCulloch and Pitts neuron by @mcculloch43a in \citeyear{mcculloch43a}. Based upon that work \citeauthor{rosenbaltt1957perceptron} made one of the earliest attempts to replicate a biological neuron using digital circuits [@rosenbaltt1957perceptron, circa 1957]. The effectiveness of the perceptron was criticized by \citeauthor{minsky1969introduction} [@minsky1969introduction] leading to the work being largely ignored and further research to stop until much later.

Today we would describe the perceptron as a binary linear classifier for use in supervised learning. As far as application of perceptron models go normal single layer perceptrons don't see much practical usage due to their simplicity and shortcommings involving non-linearly separable datasets, but they serve as the foundations of other models such as Multi-Layer Perceptrons or Neural Networks. They also see use in educational contexts exactly because of their simplicity to serve as an introduction to machine learning models.

In simple terms it's a simplified model of brain neurons. It works by evaluating a function $f(x)$:

\begin{equation}
f(x) = 
\begin{cases}
1, & \sum\limits_{i=0}^{n} w_i \cdot x_i  + b > 0 \\
0, & \text{otherwise}
\end{cases}
\end{equation}

where $w$ is a vector of $n$ real valued weights $w_0, w_1, \unicodeellipsis x_n$

x is a vector of $n$ real valued samples $x_0, x_1, \unicodeellipsis x_n$

$n$ is the number of inputs to the perceptron,

and $b$ is a bias term used to shift the activation boundary away from the origin.

#### Training Perceptrons

Training a perceptron involves a number of steps and the use of a label vector $y$ of $m$ elements $y_0, y_1 \unicodeellipsis y_m$ corresponding to each sample vector $x$. First the values for the $w$ and $b$ are chosen at random. Afterwards, for an input vector $x$, each sample is multiplied with it's coresponding weight in $w$, and the results plus the $b$ are summed up. Then the activation function $f(x)$ is used on the result and compaired to the coresponding label in $y$ in order to calculate a prediction error using a loss function [@hastie2001elements, pages 349-350]. Using the prediction error, $w$ and $b$ are adjusted. This is repeated for all $m$ input vectors until the perceptron has reached
a satisfactory prediction accuracy.

### Neural Networks

Neural Networks are a wide category of models but in general can be simply thought of as a network of perceptrons [@neural-networks]. Many variants exist such as Deep Neural Networks [@deep-learning] for more complex classification problems and Convolutional Neural Networks [@cnn] for classification of images. Being such a wide category, Neural Networks have wide applications in all classification tasks mentioned in \nameref{sec:class}.

 They are comprised by three types of layers:

1. The input layer, with $n$ neurons, one for each input sample.

2. The hidden layers, with an arbitrary number of neurons for each layer.

3. The output layer, that can include:
    1. only one neuron for simple binary classification
    2. multiple ones for confidence based multiclass classification
    3. many more for use in other non-classification applications, as for example image generation where the output layer might include as many or more neurons as the input layer.

#### Training Neural Networks

The details of training a neural network depend on it's exact variant but the general case is as follows. First all the weights and bias term are chosen at random. Then the model is an input, with each sample going throught the network's neurons's activation functions. And finally the results are consolidated in the output layer. Now using backpropagation [@schmidhuber2022annotated, pages 9-10] and
a loss function [@hastie2001elements, pages 349-350] the weights and biases of each neuron are recalculated. This is repeated for many inputs until a satisfactory model has been trained.


### K-Nearest Neighbor

K-Nearest Neighbor also known as k-NN or KNN is an unsupervised learning technique for use in classification that works by finding the $k$ nearest neighbors of an input vector in the feature space and classifies inputs (also called the queries) according the the most common label of the found neighbors. A "1"-Nearest Neightbor algorithm is descripbed by @onenearest.
Of note is the fact that training a k-NN model equals storing the training samples, a fact that restricts a naive implementation of the model based on the available memory and the size of the training dataset. Non-naive implementations include k-D Trees [@kdtree] and Ball Trees [@omohundro1989five] that attempt to deal with the inefficiencies of the algorithm.
It is loosly related and not to be confused with the Kmeans clustering algorithm [@macqueen1967some].

#### Training and Prediction with k-NNs

As mentioned before, training such a model is very simple as it only involves storing the training inputs and their coresponding labels. The important part is correctly choosing an $k$ for the number of neighbors and the distance metric ---both depend on the dataset used. Selecting the value for $k$ requires the use of heuristic hyperparameter optimization techniques, which will be covered in !SOMEWHERE! <!-- TODO: reference the section from SVM -->. Appart from hyperparameter optimization, it can also benefit from dimensionallity reduction of the feature space, which will be also covered in <!-- TODO: above -->.
The distance metric can be any Minkowski distance or in the case of text classfication a Levenshtein distance [@Levenshtein] can be used, again it highly depends on the shape of the data. Afterwards for a chosen $N$, predicting the class of an input is as simple as finding the $N$ closest training inputs

### Naive Bayes

Naive Bayes is a classifier based on the Bayes Theorem \ref{eq:bt} and the "naive" assumption that the features of the classes are independent of one another.

\begin{equation}
\label{eq:bt}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{equation}


From \ref{eq:bt} according to @zhang2004optimality

we can say that the probability that a sample $x$ with features $<x_1, x_2, ..., x_n>$ belongs in class $c$ is:

\begin{equation}
P(c|x) = \frac{P(x|c)P(c)}{P(x)}
\end{equation}

And $x$ is classified in class $C$ iif:

\begin{equation}
f_b(x) = \frac{P(C = +|x)}{P(C = -|x)} \geq 1,
\end{equation}

Again given the naive assumption that features are independent:

\begin{equation}
\label{eq:nb}
f_nb(x) = \frac{P(C = +|x)}{P(C = -|x)} \prod_{i=1}^{n}\frac{P(x_i|C = +)}{P(x_i| C = -)}
\end{equation}

As such the function $f_nb(x)$ is the Naive Bayes Classifier.

#### Types of NB Classifiers

* Gaussian

* Multinomial

* Bernoulli

* Complement

* Categorical

<!-- Gaussian Naive Bayes: gaussiannb is used in classification tasks and it assumes that feature values follow a gaussian distribution. -->
<!-- Multinomial Naive Bayes: It is used for discrete counts. For example, let’s say,  we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of it as “number of times outcome number x_i is observed over the n trials”. -->
<!-- Bernoulli Naive Bayes: The binomial model is useful if your feature vectors are boolean (i.e. zeros and ones). One application would be text classification with ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively. -->
<!-- Complement Naive Bayes: It is an adaptation of Multinomial NB where the complement of each class is used to calculate the model weights. So, this is suitable for imbalanced data sets and often outperforms the MNB on text classification tasks. -->
<!-- Categorical Naive Bayes: Categorical Naive Bayes is useful if the features are categorically distributed. We have to encode the categorical variable in the numeric format using the ordinal encoder for using this algorithm. -->


#### Training and Prediction with Naive Bayes

A Naive Bayes model is "trained" throught calculating the probabilities needed. First the a priori probabilities $P(c)$ of each class are calculated using their frequency of appearance in the training set:

\begin{equation}
\label{eq:apriory_freq}
P(c_i)= \frac{n_i}{N}
\end{equation}

$n_i$ is the number of samples of class $c_i$
and $N$ is the total number of samples.

Then, depending on the exact type of Naive Bayes used, the probability of each feature given a class is calcutated.

A kernel function to estimate the probability of (?) TODO may be used to improve it's performance much like it's used for dimensionallity reduction in <!-- TODO: ref -->

[@zhang2004optimality]

### SVM

Support Vector Machines is a linear binary classifier that works by attempting to find the hyperplane separating two classes.

#### Kernel Trick

As a linear classifier, a linearly separable dataset is usually required \ref{sec:linear}. If instead(?instead of what) a
