
# Classification

Classification is the process of assigning a class label to each sample of a dataset, an algorithm that classifies data is called a Classifier. It has a wide range of applications ranging from medical diagnosis [@medical] to natural language processing [@text] and building recognition [@alex] to financial fraud detection [@fraud]. As such lots of research has gone into improving classification accuracy (as well as minimizing run time)(?). This section will cover the training of classifiers, various categories of classifiers and finally various types of classifiers used today.

## Learning Strategies(?)

Classification is usually(?) split into two distinct phases, the training phase: wherein the classifier learns how to classify samples, and the prediction phase: wherein the classifier actually attempts to classify a dataset. But depending on when the training phase happens there exist multiple training strategies.

### Batch Learning {#sec:batch}

In batch training, the training and prediction phases **are** distinct and the training happens all at once--- before the prediction stage.

#### Training Phase

The first phase is the training phase wherein the classifier will be given a dataset that is representative of the data it's meant to later classify.

##### Supervised learning

The datasets used in training are usually annotated with the class label of each data point.
In supervised learning [@supervised], the dataset used in training will be annotated with the class label of each sample. In this way the classifier can learn to extract sets of features that are representative of each class by summarizing(?) the common features of samples with the same class label.

##### Unsupervised learning

Techniques that can operate on unlabeled datasets exist and fall under the label of unsupervised learning [@unsupervised].
In unsupervised learning, the dataset used in training is not annotated. Unsupervised learning techniques are used when a training dataset does not exist or is prohibitively expensive, time consuming or inconvenient to annotate correctly [@annotation].

#### Prediction Phase

The prediction phase is what happens after successful training of a classifier. The classifier is given a number of samples and assigns each a class label or a probability for each class label.

### Online learning

With online learning, as opposed to \nameref{sec:batch}, the classifier is designed to learn on $\ell + 1$ samples, where $\ell$ is the number of samples already having been used for training. In other words the classifier is given an unlabeled sample, it makes a prediction, and then the classifier is given the correct label which it uses to update it's model [@online-slides, page 20]. Online learning can be used in conjunction with either unsupervised and supervised learning. Online learning is useful when the training data is prohibitively large [@online], is generated in real time [@online-realtime] or when training has to be interleaved with prediction.

## Binary and Multiclass {#sec:bmclass}

Depending on the number of classes in a dataset there exist two kinds of classification. Binary classification is when the number of classes is exactly two and multiclass classification is for more than two classes. Not all kinds of classifiers do inherently support multiclass classification, but one can implement multiclass classifiers by using multiple binary classifiers. There are two main methods of extending binary classifiers to be used in multiclass classification.

### One vs All

One vs All^[One vs All: Also known as One vs Rest (OVR), One Against All (OAA) or One Against Rest (OAR)] (OVA) [@ova-ovo] multiclass classifiers have a classifier for each class. For $n$ classes, $n$ binary classifiers are required, one for each class. Each binary classifier distinguishes whether a sample belongs to a specific class or not (or any of the rest)(?). To assign the final class label to the sample the class label with the highest prediction confidence is used.

### One vs One

One vs One^[One vs one: Also known as One Against One (OAO)] (OVO) [@ova-ovo] multiclass classifiers have a classifier for every 2-combination^[2-combination: a distinct selection of 2 elements from a set] of classes, for $n$ classes $n \choose 2$ binary classifiers are required. 

\begin{equation}
\binom{n}{2} = \frac{n (n - 2 + 1)}{2 (2 - 1) 1} = \frac{n(n-1)}{2}
\end{equation}

Each binary classifier labels whether a sample belongs to one of it's two classes classes. The final class label is assigned to a sample using the class label predicted by the majority of the binary classifiers.

## Linear Classifiers {#sec:linear}

A linear classifier is a classifier that can only correctly predict the class of all it's inputs correctly if the dataset is linearly separable.
For a dataset to be linearly separable there must exist at least one hyperplane^[a subset with dimension $n-1$ where $n$ is the dimension of the dataset] that can create two half-spaces where each one only contains one class of samples(?).

more abstractly a set $X$ with distinct subsets $X_0$ and $X_1$ is said to be linearly separable if there exist $n+1$, $w_1, w_2, \unicodeellipsis, w_n, k$ where $w_i, k \in \BbbR$ satisfying

\begin{align}
\sum^{n}_{i=1} w_i x_i > k & \\
\sum^{n}_{i=1} w_i x_i < k &
\end{align}

where $x_i$ is the $i\textrm{-th}$ element of $X_0$ and $X_1$ respectively.

Non linear classifiers do not have this limitation.

## Classifiers

This subsection will go over a number of classifiers, as well as (their way of operation)(?). It will also go over the basics of SVM, but more details will be given in \nameref{sec:SVM}

### Perceptron

The perceptron is a binary linear classifier used in supervised learning <!-- TODO: reference -->. 
It's a simplified model of brain neurons. It works by evaluating a function $f(x)$:

\begin{equation}
f(x) = 
\begin{cases}
1, & \sum\limits_{i=0}^{n} w_i \cdot x_i  + b > 0 \\
0, & \text{otherwise}
\end{cases}
\end{equation}

where $w$ is a vector of $n$ real valued weights $w_0, w_1, \unicodeellipsis x_n$

x is a vector of $n$ real valued samples $x_0, x_1, \unicodeellipsis x_n$

$n$ is the number of inputs to the perceptron,

and $b$ is a bias term used to shift the activation boundary away from the origin.

#### Training

#### Applications

### Neural Networks

Neural networks can be simply thought of as a network of perceptrons [@neural-networks]. Many variants exist such as Deep Neural Networks [@deep-learning] for more complex classification problems and Convolutional Neural Networks [@cnn] for classification of images.

#### Training ?

#### Applications

### K-Nearest Neighbor


how

#### Training

#### Applications

### Naive Bayes

Naive Bayes is a classifier based on the Bayes Theorem \ref{eq:bt} and the "naive" assumption that the features of the classes are independent.

\begin{equation}
\label{eq:bt}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{equation}

#### Applications

### SVM

Support Vector Machines is a linear binary classifier that works by attempting to find the hyperplane separating two classes.

#### Kernel Trick

As a linear classifier, a linearly separable dataset is usually required \ref{sec:linear}. If instead(?instead of what) a
