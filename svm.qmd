### SVM

Support Vector Machines is a linear binary classifier that works by attempting to find the maximally separating hyperplane between two classes. A hyperplane is an $n-1$ dimensional vector where $n$ is the dimensionality of the feature space.

<!-- TODO: Hyperplane image -->

<!-- \coordinate (i) at (800/9,2600/27); -->
<!-- \coordinate (j) at (800/9,0); -->
<!-- \draw (-5,0)--(100,0); -->
<!-- \draw (0,-5)--(0,125); -->
<!-- \draw[thick,domain=0:100] plot (\x,{(7*\x/4-3*(\x/10)*(\x/10)/4}); -->
<!-- \draw[thick,domain=0:100] plot (\x,{(3*\x/4+3*(\x/10)*(\x/10)/8}); -->
<!-- \draw[dashed] (j)--(i); -->
<!-- \node[above left] at (60,78) {$7000x-30x^2$}; -->
<!-- \node[above left] at (100,112) {$3000x+15x^2$}; -->
<!-- \node at (50,60) {$A$}; -->
<!-- \node at (i) {$\bullet$}; -->
<!-- \node at (0,0) {$\bullet$}; -->
<!-- \node[below left] at (0,0) {$0$}; -->
<!-- \node[below] at (j) {$\frac{800}9$}; -->
<!-- \node at (100, 100) {$\bullet$}; -->
<!-- \node at (90, 95) {$\bullet$}; -->
<!-- \node at (90, 105) {$\bullet$}; -->
<!-- \node at (105, 95) {$\bullet$}; -->
<!-- \node at (15, 25) {$\bullet$}; -->
<!-- \node at (30, 35) {$\bullet$}; -->
<!-- \node at (35, 30) {$\bullet$}; -->
<!-- \node at (35, 45) {$\bullet$}; -->

<!-- \begin{tikzpicture}[scale=1] -->
<!-- 	\draw plot [only marks, mark=*, mark size=0.5, domain=0:5, samples=700] -->
<!-- 	({10 + rnd },{10 + rnd}); -->
<!-- \end{tikzpicture} -->


#### The Primal Problem

More formally an SVM classifier tries to solve a constrained optimization problem. We will now prove that SVM's problem is a constraint optimization problem.
We will define $w$ to be the separating hyperplane, $b$ a bias term and $x$ a vector of $n$ feature samples. 

<!-- TODO: start with earlier SVM definition -->

\begin{equation}
\min\limits_{w,b} \frac{||W||^2}{2}
\end{equation}

Subject to:
\begin{align*}
y_i(w^T +b) >= 1 \text{for } i=1,...,n
\end{align*}

The problem can also be expressed as a general convex optimization problem of the following form:
\begin{equation}
\min\limits_{x} f(x)
\end{equation}

Subject to:
\begin{align*}
g_i(x) \leq 0, \text{for } i=1,...,n
\end{align*}

Substituting with:
\begin{align*}
f(w,b) =& \frac{||W||^2}{2} \\
g_{i}(w,b) \leq& 0 \\
g_{i}(w,b) =& 1 - y_i(w^T x_i + b) \\
\end{align*}

As such we can finally write the problem as:
\begin{equation}
\min\limits_{w,b}f(w,b), \text{where } f(w,b) = \frac{||W||^2}{2}
\end{equation}

Subject to:
\begin{align*}
g_{i}(w,b) \leq& 0 \\
g_{i}(w,b) =& 1 - y_i(w^T x_i + b) \\
\end{align*}

But for this to actually be a convex optimization problem, we will need to show that $f$ and all $g_i$ are convex functions.

For $f$ it is sufficient to show that it's Hessian matrix is positive semidefinite (PSD). For a matrix to be PSD it is sufficient to show that $\forall z$ $z^THz>=0$.

\begin{proof}
The Hessian matrix is defined as such:

\begin{equation}
H =
\begin{bmatrix}
  \dfrac{\partial^2 f}{\partial x_1^2} & \dfrac{\partial^2 f}{\partial x_1\,\partial x_2} & \cdots & \dfrac{\partial^2 f}{\partial x_1\,\partial x_n} \\[2.2ex]
  \dfrac{\partial^2 f}{\partial x_2\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_2^2} & \cdots & \dfrac{\partial^2 f}{\partial x_2\,\partial x_n} \\[2.2ex]
  \vdots & \vdots & \ddots & \vdots \\[2.2ex]
  \dfrac{\partial^2 f}{\partial x_n\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_n\,\partial x_2} & \cdots & \dfrac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
\end{equation}

As such for $f$, it's Hessian is:

\begin{equation}
H = \begin{bmatrix} 
    \frac{\partial\nicefrac{1}{2}||W||^2}{\partial w_1^2} & \dots  & \frac{\partial\nicefrac{1}{2}||W||^2}{\partial w_1 \partial w_d}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial\nicefrac{1}{2}||W||^2}{\partial w_d \partial w_1} & \dots  &\frac{\partial\nicefrac{1}{2}||W||^2}{\partial w_d \partial w_d} 
    \end{bmatrix}
\end{equation}

Knowing that $\nicefrac{1}{2} ||W||^2 = \nicefrac{1}{2} \sum_{i=1}^{d} w_i^2$, we can compute the first partial derivative and get:

\begin{equation}
H = \begin{bmatrix} 
    \frac{\partial w_1}{\partial w_1} & \dots  & \frac{\partial w_1}{\partial w_d}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial w_d}{\partial w_1} & \dots  &\frac{\partial w_d}{\partial w_d} 
    \end{bmatrix}
\end{equation}

Computing the second partial derivative gets us:

\begin{equation}
H = \begin{bmatrix} 
    1 & \dots  & 0\\
    \vdots & \ddots & \vdots\\
    0 & \dots  & 1
    \end{bmatrix} = I
\end{equation}

Which is obviously the identity matrix. As such:

\begin{align*}
z^T H z \geq& 0 \Rightarrow \\
z^T I z \geq& 0 \Rightarrow \\
z^T z \geq &0 \Rightarrow \\
\sum_{i=1}^d z_i^2 \geq& 0
\end{align*}

Which clearly holds true.
\end{proof}

Similarly we will show that all $g_i$ are convex, because their Hessians are PSD.

\begin{proof}

\begin{align}
H &= \begin{bmatrix}
		\frac{\partial^2(1-y_i(w^Tx_i + b))}{\partial w_1 \partial w_d} & \dots  & \frac{\partial^2(1-y_i(w^Tx_i + b))}{\partial w_d \partial b} \\
		\vdots & \ddots & \vdots \\
		\frac{\partial^2(1-y_i(w^Tx_i + b))}{\partial b \partial w_1} & \dots  &\frac{\partial^2(1-y_i(w^Tx_i + b))}{\partial b \partial b}
    \end{bmatrix} \Rightarrow \\
H &= \begin{bmatrix}
		\frac{\partial - y_i x_{i,1}}{\partial w1} & \dots  & \frac{\partial - y_i x_{i,1}}{\partial b} \\
		\vdots & \ddots & \vdots\\
		\frac{\partial - y_i}{\partial w_1} & \dots  & \frac{\partial - y_i}{\partial b}
    \end{bmatrix} = I
\Rightarrow \\
H &= \begin{bmatrix}
		0 & \dots  & 0 \\
		\vdots & \ddots & \vdots\\
		0 & \dots  & 0
    \end{bmatrix}
\end{align}

For which $\forall z$, $z^THz>=0$ clearly holds true.
\end{proof}

#### The Dual Problem

To solve this "primal" optimization problem, we will make use the "dual" form of this problem. We introduce the generalized Lagrangian, defined as:

\begin{equation}
L(x,z) = f(x) + \sum_{i=1}^n a_i g_i(x)
\end{equation}

Where $a_i$ are what are known as Lagrange multipliers.

Denote:

\begin{align}
\theta_P(x) &=\max\limits_{a: a_i\geq0} L(x, a) \\
\theta_P(x) &= \max\limits_{a: a_i\geq0} f(x) + \sum_{i=1}^n a_i g_i(x)
\end{align}

Assume that an $x$, violates one primal $g_j(x) > 0$ constraint.
Since we are picking the maximum $a$, we can we let $a_j \rightarrow \infty$, we get $\theta_P(x) = \infty$.

Assume that an $x$, satisfies **all** primal $g_j(x) \leq 0$ constraint.
We can let all $a_j = 0$, and get $\theta_P(x) = f(x)$.

From this we can see that if we assume that x satisfies all primal constraints that the following has the same optimal solution as the original primal problem:

\begin{equation}
\min\limits_{x}\theta_P(x) = \min\limits_{x}\max\limits_{a: a_i\geq0} L(x,a)
\end{equation}

Let $p^* = \min\limits_{x}\theta_P(x)$ denote the **primal value**.

A dual problem is defined as a function:

\begin{equation}
\theta_D(a) = \min\limits_{x}L(x,a)
\end{equation}

and a **dual value**:

\begin{equation}
d^* = \max\limits_{a: a_i\geq0} \theta_D(a) = \max\limits_{a: a_i\geq0}\min\limits_{x}L(x,a)
\end{equation}

The maximum of the minimum of something is obviously less than or equal to the minimum of the maximum of something, and as such we can see that $d^* \leq p^*$

\newtheorem{theorem}{Theorem}

\begin{theorem}
If there exists an $x^{*} $ that solves the primal problem and an $(μ^{*}, λ^{*})$ that solves the dual problem, such that they both satisfy the Karush-Kuhn-Tucker (KKT) conditions, then the problem is said to have strong duality.
If the problem pair has strong duality, then for any solution $x^{*}$ to the primal problem and any solution $\displaystyle (\mu ^{*},\lambda ^{*})$ to the dual problem, the pair $\displaystyle x^{*},(\mu ^{*},\lambda ^{*})$ must satisfy the KKT conditions \cite{kkt}.
\end{theorem}

The KKT conditions are as follows:

\begin{alignat}{3}
&\frac{\partial L(x^*, a^*)}{\partial x_i} &= 0 \; &\forall i \in [1,d] \label{eq:KKT1} \\
&a_i^* g_i(x^*) &= 0 \; &\forall i \in [1,n] \label{eq:KKT2} \\
&g_i(x^*) &\leq 0 \; &\forall i \in [1,n] \label{eq:KKT3}  \\
&a_i^* &\geq 0 \; &\forall i \in [1,n] \label{eq:KKT4}
\end{alignat}

Where all $i$ are $\in \mathbb{N}$

If $f$ and all $g_i$ are convex and the $g_i$ constraints are strictly feasible^[strictly feasible: $\exists x, g_i(x)<0, \forall i$] it is trivial to prove that $d^* = p^*$.

We will now show that the $g_i$ constraints are strictly feasible:

\begin{proof}
If we assume we have a linearly separable dataset then a separating hyperplane $w^Tx+b$ should correctly classify all samples, in other words $y_i(w^Tx+i+b)>0 \forall i$.

As such we could scale $w$ and $b$ by an arbitrary number in order for $g_{i}(w,b) < 0$ to hold true, where $g_{i}(w,b) = 1 - y_i(w^T x_i + b)$.
\end{proof}

We will now attempt to solve the dual form of the problem:

\begin{align}
&\max\limits_{a: a_i \geq 0} \min\limits_{w,b} f(w,b) + \sum_{i=1}^n a_i g_i(w,b) \label{eq:dual1} \\
&\max\limits_{a: a_i \geq 0} \min\limits_{w,b} \nicefrac{1}{2} ||W||^2 - \sum_{i=1}^n a_i (y_i(w^T x_i + b) - 1) \label{eq:dual2}
\end{align}

From \autoref{eq:KKT1} we know that all the partial derivatives of the Generalized Lagrangian will equal 0. Taking the partial derivative in respect to $w_j$:

\begin{align}
\frac{\partial \nicefrac{1}{2} ||W||^2 - \sum_{i=1}^n a_i(y_i(w^T x_i + b) -1)}{\partial w_j} &= 0 \Rightarrow \\
w_j - \sum_{i=1}^n a_i y_i x_{i,j} &= 0 \Rightarrow \\
w &= \sum_{i=1}^n a_i y_i x_{i,j} \label{eq:dual_w}
\end{align}

Substituting \autoref{eq:dual_w} into \autoref{eq:dual2}:

\begin{equation}
\max\limits_{a: a_i \geq 0} \min\limits_{w,b} \nicefrac{1}{2} ||W||^2 - \sum_{i=1}^n a_i (y_i((\sum_{j=1}^n a_j y_j x_j)^T x_i + b) - 1) \label{eq:dual3}
\end{equation}

Expanding it:

\begin{align}
&\max\limits_{a: a_i \geq 0} \min\limits_{w,b} \nicefrac{1}{2} ||\sum_{i=1}^n a_i y_i x_i||^2 - \sum_{i=1}^n a_i (y_i((\sum_{j=1}^n a_j y_j x_j)^T x_i + b) - 1) \\
&\max\limits_{a: a_i \geq 0} \min\limits_{w,b} \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j x_i^T x_j - \sum_{i=1}^n a_i (y_i((\sum_{j=1}^n a_j y_j x_j)^T x_i + b) - 1) \\
&\max\limits_{a: a_i \geq 0} \min\limits_{w,b} \sum_{i=1}^n a_i - \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j x_i^T x_j - b \sum_{i=1}^n a_i y_i \label{eq:dual4}
\end{align}

Again from \autoref{eq:KKT1} we know that all the partial derivatives of the Generalized Lagrangian will equal 0. Now taking the partial derivative in respect to $b$:

\begin{align}
\frac{\partial \nicefrac{1}{2} ||W||^2 - \sum_{i=1}^n a_i(y_i(w^T x_i + b) -1)}{\partial b} &= 0 \Rightarrow \\
-\sum_{i=1}^n a_i y_i &= 0 \label{eq:dual_zero}
\end{align}

Substituting \autoref{eq:dual_zero} into \autoref{eq:dual4}:

\begin{equation}
\max\limits_{a: a_i \geq 0} \min\limits_{w,b} \sum_{i=1}^n a_i - \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j x_i^T x_j \label{eq:dual_no_zero}
\end{equation}

Note that \autoref{eq:dual_no_zero} no longer includes neither $w$, nor $b$, so it can be safely expressed without the minimum. As such we finally get the dual form of the SVM problem that we can solve:

\begin{equation}
\max\limits_{a: a_i \geq 0} \sum_{i=1}^n a_i - \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j x_i^T x_j \label{eq:dual_final}
\end{equation}

Subject to:

\begin{equation}
\sum_{i=1}^n a_i y_i = 0
\end{equation}

##### Recovering optimal bias parameter

The optimal value for $b$ must be one that pushes the separating hyperplane to sit between the furthest support vector in $w$'s direction and the closest support vector of the other class. In other words their functional margins $y_i(w^Tx_i+b)$ must be equal:

\begin{align}
\min\limits_{i:y_i=1} w^T x_i + b &= - (\max\limits_{i:y_i=-1} w^T x_i + b) \Rightarrow \\
b &= - \frac{1}{2}(\min\limits_{i:y_i=1} w^T x_i + \max\limits_{i:y_i=-1} w^T x_i)
\end{align}

#### Kernel Trick  {#sec:kernel}

As a linear classifier, a linearly separable dataset is usually required \ref{sec:linear}. But what we can do is perform a feature transform to a space where it is linearly separable.

<!-- TODO: image of distance from the origin example -->


\begin{theorem}
Mercer's theorem. Let $x \in \mathbb{R}^l$ and a mapping $\phi$:
\begin{equation}
x \mapsto \phi(x) \in H
\end{equation}
where $H$ is a Hilbert space. The inner product operation has an equivalent representation.
\begin{equation}
<\phi(x),\phi(z)> = K(x,z)
\end{equation}
where $<\cdot,\cdot>$ denotes the inner product operation in $H$ and $K(x,z)$ is a symmetric continuous function satisfying the following conditions (known as Mercer's conditions):
\begin{equation}
\int_C \int_C K(x,z)g(x)g(z)dx dz \geq 0
\end{equation}
for any $g(x), x \in C \subset \mathbb{R}^l$ such that:
\begin{equation}
\int_C g(x)^2 dx < + \infty
\end{equation}
where C is a compact (finite) subset of $\mathbb{R}^l$.
\cite{precog}
\end{theorem}

From Mercer's theorem we can assume that a mapping $\phi: R^d \rightarrow R^{d\prime}$ from $R^d$ to $R^{d\prime}$ has an equivalent kernel function $K$. We can now apply the mapping to the training dataset $X$ before training begins and get a dataset $X\prime$. Then run SVM to find a separating hyperplane on the new $X\prime$ dataset. We will now need to first apply the transform to any new data points, before we can make predictions on them.


Essentially we have replaced the inner product of the feature vectors with a kernel function $K(x_1, x_2) = \phi(x_1)^T\phi(x_2)$. With this in mind we can rewrite \autoref{eq:dual_final}:

\begin{equation}
\max\limits_{a: a_i \geq 0} \sum_{i=1}^n a_i - \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j K(x_i, x_j) \label{eq:dual_kernel}
\end{equation}

Subject to:

\begin{equation}
\sum_{i=1}^n a_i y_i = 0
\end{equation}

With:

\begin{align*}
b =& -\frac{1}{2}(\min\limits_{i:y_i=1} \sum_{a_j \neq 0} a_j y_j K(x_i, x_j) + \max\limits_{i:y_i=-1} \sum_{a_j \neq 0} a_j y_j K(x_i, x_j)) \\
w =& \sum_{i=1}^n a_i y_i \phi(x_i)
\end{align*}

There are many kernels that are used with SVM:

* Linear kernel: $<x,\prime(x)>$

* Polynomial kernel: $(\gamma<x,\prime(x)> + r)^d$ where $d$ is the degree of the polynomial and $r$, $\gamma$ bias parameters.

* Exponential kernel: $exp(-\gamma||x-\prime(x)||^2)$, where $\gamma$ is a bias term.

* Radial Basis Function (RBF) kernel: $exp(-\gamma ||x — x\prime||^2)$, where $\gamma$ is a bias term.

#### Regularisation

For some non linearly separable datasets, the non separability might be because of a few outliers in the dataset. In this case using a feature transform might not be the best way to deal with the dataset. What we can do instead is allow a few samples to be misclassified by adding slack variables. The primal form with slack variables is defined as such:

\begin{equation}
\min\limits_{w,b,\xi} \nicefrac{1}{2}||W||^2 + C \sum_{i=1}^n \xi_i
\end{equation}

Subject to:

\begin{align}
y_i(w^T x_i + b) \geq 1 - \xi_i \; &\forall i \in [1,n] \\
\xi_i \geq 0 \; &\forall i \in [1,n] 
\end{align}

Where $\xi_i$s are the slack variables, and $C$ is the misclassification "cost".

Following the same methodology as before it is trivial to show that the problem function remains convex and the constraints linear.

The dual form of the regularized SVM problem is defined as:

\begin{align}
\max\limits_{a,r \geq 0} \min\limits{w,b,\xi} & L(w,b,\xi, a, r) \\
\max\limits_{a,r \geq 0} \min\limits{w,b,\xi} & \nicefrac{1}{2} ||W||^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n a_i (y_i(w^T x_i + b) - 1 + \xi_i) - \sum_{i=1}^n r_i \xi_i \label{eq:dual_reg}
\end{align}

Subject to:

\begin{align}
y_i(w^T x_i + b) \geq 1 - \xi_i \; &\forall i \in [1,n] \\
\xi_i \geq 0 \; &\forall i \in [1,n] 
\end{align}

From \autoref{eq:KKT1} again, we know the partial derivatives of the generalized Lagrangian will be equal to zero. Taking the partial derivative of \autoref{eq:dual_reg} in respect to $w_j$:

\begin{align}
\frac{\partial L(w,b,\xi,a,r)}{\partial w_j} &= 0 \Rightarrow \\
\frac{\partial \nicefrac{1}{2} ||W||^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n a_i (y_i(w^T x_i + b) - 1 + \xi_i) - \sum_{i=1}^n r_i \xi_i}{\partial w_j} &= 0 \Rightarrow \\
w_j - \sum_{i=1}^n a_i y_i x_{i,j} &= 0 \Rightarrow \\
w &= \sum_{i=1}^n a_i y_i x_{i,j}
\end{align}

So $w$ remains unchanged.

Taking the partial derivative of \autoref{eq:dual_reg} with respect to $b$:

\begin{align}
\frac{\partial L(w,b,\xi,a,r)}{\partial b} &= 0 \Rightarrow \\
\frac{\partial \nicefrac{1}{2} ||W||^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n a_i (y_i(w^T x_i + b) - 1 + \xi_i) - \sum_{i=1}^n r_i \xi_i}{\partial b} &= -\sum_{i=1}^n a_i y_i \Rightarrow \\
\sum_{i=1}^n a_i y_i &= 0
\end{align}

Again this remains unchanged.

Taking the partial derivative of \autoref{eq:dual_reg} in respect to $\xi_i$:

\begin{align}
\frac{\partial L(w,b,\xi,a,r)}{\partial \xi_i} &= 0 \Rightarrow \\
\frac{\partial \nicefrac{1}{2} ||W||^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n a_i (y_i(w^T x_i + b) - 1 + \xi_i) - \sum_{i=1}^n r_i \xi_i}{\partial \xi_i} &= -\sum_{i=1}^n a_i y_i \Rightarrow \\
C - a_i - r_i &= 0 \Rightarrow \\
C &= a_i + r_i
\end{align}


Now by substituting the above results into \autoref{eq:dual_reg} the dual regularized form can be rewritten as:

\begin{equation}
\max\limits_{a,r \geq 0} \min\limits{w,b,\xi} \sum_{i=1}^n a_i - \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j x_i^T x_j + \sum_{i=1}^n(C- a_i - r_i) \xi_i
\end{equation}

Subject to:

\begin{align*}
&\sum_{i=1}^n a_i y_i = 0 \\
&C = a_i + r_i \; \forall i \in [1,n]
\end{align*}

We can now note that $r$ does not appear in the problem function and that we can always choose $r_i \geq 0$ such that $C=a_i + r_i$ as long as $a_i  \leq C$. We can also note that $w$, $b$ nor $\xi$ appear. Rewriting the regularized dual form again yields:


\begin{equation}
\max\limits_{a} \sum_{i=1}^n a_i - \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j x_i^T x_j \label{eq:dual_reg_final}
\end{equation}

Subject to:

\begin{align*}
&\sum_{i=1}^n a_i y_i = 0 \label{eq:lincon} \\
& 0 \leq a_i \leq C \; \forall i \in [1,n]
\end{align*}

The kernel trick from \autonameref{sec:kernel}, still applies.

# Improvements to SVM {#sec:SVM}

The dual form we have come upon in \autoref{eq:dual_final}, as well as the kernelized form in \autoref{eq:dual_kernel} can be solved using standard quadratic programming (QP) solvers. Using those can be very performance and memory intensive. For that reason many techniques where developed to speed up SVM.

## Sequential Minimization Optimization

One of the more successful ones was Sequential Minimization Optimization (SMO) which takes the relatively large QP problem of SVM, breaks it into many smaller ones, and solves them analytically [@smo].

For each optimization step due to \autoref{eq:lincon}, two Lagrange multipliers are jointly optimized at a time.

## Cascade SVM

## Extreme Cascade Machines ?

## GPUSVM
