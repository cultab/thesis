### SVM

Support Vector Machines is a linear binary classifier that works by attempting to find the maximally separating hyperplane between two classes. A hyperplane is an $n-1$ dimensional vector where $n$ is the dimensionality of the feature space.

![Maximum-margin hyperplane and margin for an SVM trained on two classes. Larhmam / [CC BY-SA 4.0 DEED](https://creativecommons.org/licenses/by-sa/4.0/deed.en)](./img/svm_margin.png){width=50%}

<!-- \coordinate (i) at (800/9,2600/27); -->
<!-- \coordinate (j) at (800/9,0); -->
<!-- \draw (-5,0)--(100,0); -->
<!-- \draw (0,-5)--(0,125); -->
<!-- \draw[thick,domain=0:100] plot (\x,{(7*\x/4-3*(\x/10)*(\x/10)/4}); -->
<!-- \draw[thick,domain=0:100] plot (\x,{(3*\x/4+3*(\x/10)*(\x/10)/8}); -->
<!-- \draw[dashed] (j)--(i); -->
<!-- \node[above left] at (60,78) {$7000x-30x^2$}; -->
<!-- \node[above left] at (100,112) {$3000x+15x^2$}; -->
<!-- \node at (50,60) {$A$}; -->
<!-- \node at (i) {$\bullet$}; -->
<!-- \node at (0,0) {$\bullet$}; -->
<!-- \node[below left] at (0,0) {$0$}; -->
<!-- \node[below] at (j) {$\frac{800}9$}; -->
<!-- \node at (100, 100) {$\bullet$}; -->
<!-- \node at (90, 95) {$\bullet$}; -->
<!-- \node at (90, 105) {$\bullet$}; -->
<!-- \node at (105, 95) {$\bullet$}; -->
<!-- \node at (15, 25) {$\bullet$}; -->
<!-- \node at (30, 35) {$\bullet$}; -->
<!-- \node at (35, 30) {$\bullet$}; -->
<!-- \node at (35, 45) {$\bullet$}; -->

<!-- \begin{tikzpicture}[scale=1] -->
<!-- 	\draw plot [only marks, mark=*, mark size=0.5, domain=0:5, samples=700] -->
<!-- 	({10 + rnd },{10 + rnd}); -->
<!-- \end{tikzpicture} -->


#### The Primal Problem

More formally an SVM classifier tries to solve a constrained optimization problem. This section will now prove that SVM's problem is a constraint optimization problem.
Let $w$ to be the separating hyperplane, $b$ a bias term and $x$ a vector of $n$ feature samples. 

<!-- TODO: start with earlier SVM definition -->

\begin{equation}\label{eq:primal}
\min\limits_{w,b} \frac{||W||^2}{2}
\end{equation}

\begin{subequations}
Subject to:
\begin{equation}
y_i(w^T +b) >= 1, \; \text{for } i=1,\ldots,n
\end{equation}
\end{subequations}

The problem can also be expressed as a general convex optimization problem of the following form:

\begin{equation}
\min\limits_{x} f(x)
\end{equation}

Subject to:

\begin{subequations}
\begin{equation}
g_i(x) \leq 0, \; \text{for } i=1,\ldots,n
\end{equation}
\end{subequations}

Substituting with:

\begin{subequations}
\begin{gather}
f(w,b) = \frac{||W||^2}{2} \\
g_{i}(w,b) = 1 - y_i(w^T x_i + b)
\end{gather}
\end{subequations}

As such the problem can finally be written as:

\begin{equation}
\min\limits_{w,b}f(w,b), \text{where } f(w,b) = \frac{||W||^2}{2}
\end{equation}

Subject to:
\begin{subequations}
\begin{gather}
g_{i}(w,b) \leq 0 \\
g_{i}(w,b) = 1 - y_i(w^T x_i + b) \\
\end{gather}
\end{subequations}

But for this to actually be a convex optimization problem, it will need to be shown that $f$ and all $g_i$ are convex functions.

For $f$ it is sufficient to show that it's Hessian matrix is positive semidefinite (PSD). For a matrix to be PSD it is sufficient to show that $\forall z$ $z^THz>=0$.

\begin{proof}
The Hessian matrix is defined as such:

\begin{equation}
H =
\begin{bmatrix}
  \dfrac{\partial^2 f}{\partial x_1^2} & \dfrac{\partial^2 f}{\partial x_1\,\partial x_2} & \cdots & \dfrac{\partial^2 f}{\partial x_1\,\partial x_n} \\[2.2ex]
  \dfrac{\partial^2 f}{\partial x_2\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_2^2} & \cdots & \dfrac{\partial^2 f}{\partial x_2\,\partial x_n} \\[2.2ex]
  \vdots & \vdots & \ddots & \vdots \\[2.2ex]
  \dfrac{\partial^2 f}{\partial x_n\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_n\,\partial x_2} & \cdots & \dfrac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
\end{equation}

As such for $f$, it's Hessian is:

\begin{equation}
H = \begin{bmatrix} 
    \frac{\partial\nicefrac{1}{2}||W||^2}{\partial w_1^2} & \dots  & \frac{\partial\nicefrac{1}{2}||W||^2}{\partial w_1 \partial w_d}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial\nicefrac{1}{2}||W||^2}{\partial w_d \partial w_1} & \dots  &\frac{\partial\nicefrac{1}{2}||W||^2}{\partial w_d \partial w_d} 
    \end{bmatrix}
\end{equation}

Knowing that $\nicefrac{1}{2} ||W||^2 = \nicefrac{1}{2} \sum_{i=1}^{d} w_i^2$, the first partial derivative can be computed to get:

\begin{equation}
H = \begin{bmatrix} 
    \frac{\partial w_1}{\partial w_1} & \dots  & \frac{\partial w_1}{\partial w_d}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial w_d}{\partial w_1} & \dots  &\frac{\partial w_d}{\partial w_d} 
    \end{bmatrix}
\end{equation}

Computing the second partial derivative gets us:

\begin{equation}
H = \begin{bmatrix} 
    1 & \dots  & 0\\
    \vdots & \ddots & \vdots\\
    0 & \dots  & 1
    \end{bmatrix} = I
\end{equation}

Which is obviously the identity matrix. As such:

\begin{align*}
z^T H z \geq& 0 \Rightarrow \\
z^T I z \geq& 0 \Rightarrow \\
z^T z \geq &0 \Rightarrow \\
\sum_{i=1}^d z_i^2 \geq& 0
\end{align*}

Which clearly holds true.
\end{proof}

Similarly it will shown that all $g_i$ are convex, because their Hessians are PSD.

\begin{proof}

\begin{align}
H &= \begin{bmatrix}
		\frac{\partial^2(1-y_i(w^Tx_i + b))}{\partial w_1 \partial w_d} & \dots  & \frac{\partial^2(1-y_i(w^Tx_i + b))}{\partial w_d \partial b} \\
		\vdots & \ddots & \vdots \\
		\frac{\partial^2(1-y_i(w^Tx_i + b))}{\partial b \partial w_1} & \dots  &\frac{\partial^2(1-y_i(w^Tx_i + b))}{\partial b \partial b}
    \end{bmatrix} \Rightarrow \\
H &= \begin{bmatrix}
		\frac{\partial - y_i x_{i,1}}{\partial w1} & \dots  & \frac{\partial - y_i x_{i,1}}{\partial b} \\
		\vdots & \ddots & \vdots\\
		\frac{\partial - y_i}{\partial w_1} & \dots  & \frac{\partial - y_i}{\partial b}
    \end{bmatrix} = I
\Rightarrow \\
H &= \begin{bmatrix}
		0 & \dots  & 0 \\
		\vdots & \ddots & \vdots\\
		0 & \dots  & 0
    \end{bmatrix}
\end{align}

For which $\forall z$, $z^THz>=0$ clearly holds true.
\end{proof}

#### The Dual Problem

The dual form of this problem will be used to solve the primal form of this optimization problem. Introducing the generalized Lagrangian, defined as:

\begin{equation}
L(x,z) = f(x) + \sum_{i=1}^n a_i g_i(x)
\end{equation}

Where $a_i$ are what are known as Lagrange multipliers.

Denote:

\begin{equation}
\begin{split}
\theta_P(x) &=\max\limits_{a: a_i\geq0} L(x, a) \\
            &= \max\limits_{a: a_i\geq0} f(x) + \sum_{i=1}^n a_i g_i(x)
\end{split}
\end{equation}

Assume that an $x$, violates one primal $g_j(x) > 0$ constraint.
Since the maximum $a$ is picked, let $a_j \rightarrow \infty$, to get $\theta_P(x) = \infty$.

Assume that an $x$, satisfies **all** primal $g_j(x) \leq 0$ constraint.
Let all $a_j = 0$, and get $\theta_P(x) = f(x)$.

From this it is shown that if we assume that x satisfies all primal constraints that the following has the same optimal solution as the original primal problem:

\begin{equation}
\min\limits_{x}\theta_P(x) = \min\limits_{x}\max\limits_{a: a_i\geq0} L(x,a)
\end{equation}

Let $p^* = \min\limits_{x}\theta_P(x)$ denote the **primal value**.

\begin{minipage}{\textwidth}
A dual problem is defined as a function:

\begin{equation}
\theta_D(a) = \min\limits_{x}L(x,a)
\end{equation}

and a \textbf{dual value}:

\begin{equation}
\begin{split}
d^* &= \max\limits_{a: a_i\geq0} \theta_D(a) \\
    &= \max\limits_{a: a_i\geq0}\min\limits_{x}L(x,a)
\end{split}
\end{equation}

\end{minipage}

The maximum of the minimum of something is obviously less than or equal to the minimum of the maximum of something, and as such we can see that $d^* \leq p^*$

\newtheorem{theorem}{Theorem}

\begin{theorem}
If there exists an $x^{*} $ that solves the primal problem and an $(μ^{*}, λ^{*})$ that solves the dual problem, such that they both satisfy the Karush-Kuhn-Tucker (KKT) conditions, then the problem is said to have strong duality.
If the problem pair has strong duality, then for any solution $x^{*}$ to the primal problem and any solution $\displaystyle (\mu ^{*},\lambda ^{*})$ to the dual problem, the pair $\displaystyle x^{*},(\mu ^{*},\lambda ^{*})$ must satisfy the KKT conditions \cite{kkt}.
\end{theorem}

The KKT conditions are as follows:

\begin{equation} \label{eq:KKT}
\begin{split}
\frac{\partial L(x^*, a^*)}{\partial x_i} = 0 \; \forall i \in 1,\ldots,n \\
a_i^* g_i(x^*) = 0 \; \forall i \in 1,\ldots,n \\
g_i(x^*) \leq 0 \; \forall i \in 1,\ldots,n \\
a_i^* \geq 0 \; \forall i \in 1,\ldots,n
\end{split}
\end{equation}

If $f$ and all $g_i$ are convex and the $g_i$ constraints are strictly feasible^[strictly feasible: $\exists x, g_i(x)<0, \forall i$] it is trivial to prove that $d^* = p^*$.

We will now show that the $g_i$ constraints are strictly feasible:

\begin{proof}
If we assume we have a linearly separable dataset then a separating hyperplane $w^Tx+b$ should correctly classify all samples, in other words $y_i(w^Tx+i+b)>0 \forall i$.

As such we could scale $w$ and $b$ by an arbitrary number in order for $g_{i}(w,b) < 0$ to hold true, where $g_{i}(w,b) = 1 - y_i(w^T x_i + b)$.
\end{proof}

We will now attempt to solve the dual form of the problem:

\begin{align}
&\max\limits_{a: a_i \geq 0} \min\limits_{w,b} f(w,b) + \sum_{i=1}^n a_i g_i(w,b) \label{eq:dual1} \\
&\max\limits_{a: a_i \geq 0} \min\limits_{w,b} \nicefrac{1}{2} ||W||^2 - \sum_{i=1}^n a_i (y_i(w^T x_i + b) - 1) \label{eq:dual2}
\end{align}

From the first KKT condition in \autoref{eq:KKT} we know that all the partial derivatives of the Generalized Lagrangian will equal 0. Taking the partial derivative in respect to $w_j$:

\begin{align}
\frac{\partial \nicefrac{1}{2} ||W||^2 - \sum_{i=1}^n a_i(y_i(w^T x_i + b) -1)}{\partial w_j} &= 0 \Rightarrow \\
w_j - \sum_{i=1}^n a_i y_i x_{i,j} &= 0 \Rightarrow \\
w &= \sum_{i=1}^n a_i y_i x_{i,j} \label{eq:dual_w}
\end{align}

Substituting \autoref{eq:dual_w} into \autoref{eq:dual2}:

\begin{equation}
\max\limits_{a: a_i \geq 0} \min\limits_{w,b} \nicefrac{1}{2} ||W||^2 - \sum_{i=1}^n a_i (y_i((\sum_{j=1}^n a_j y_j x_j)^T x_i + b) - 1) \label{eq:dual3}
\end{equation}

Expanding it:

\begin{align}
&\max\limits_{a: a_i \geq 0} \min\limits_{w,b} \nicefrac{1}{2} ||\sum_{i=1}^n a_i y_i x_i||^2 - \sum_{i=1}^n a_i (y_i((\sum_{j=1}^n a_j y_j x_j)^T x_i + b) - 1) \\
&\max\limits_{a: a_i \geq 0} \min\limits_{w,b} \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j x_i^T x_j - \sum_{i=1}^n a_i (y_i((\sum_{j=1}^n a_j y_j x_j)^T x_i + b) - 1) \\
&\max\limits_{a: a_i \geq 0} \min\limits_{w,b} \sum_{i=1}^n a_i - \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j x_i^T x_j - b \sum_{i=1}^n a_i y_i \label{eq:dual4}
\end{align}

Again from the first KKT condition in \autoref{eq:KKT} we know that all the partial derivatives of the Generalized Lagrangian will equal 0. Now taking the partial derivative in respect to $b$:

\begin{align}
\frac{\partial \nicefrac{1}{2} ||W||^2 - \sum_{i=1}^n a_i(y_i(w^T x_i + b) -1)}{\partial b} &= 0 \Rightarrow \\
-\sum_{i=1}^n a_i y_i &= 0 \label{eq:dual_zero}
\end{align}

Substituting \autoref{eq:dual_zero} into \autoref{eq:dual4}:

\begin{equation}
\max\limits_{a: a_i \geq 0} \min\limits_{w,b} \sum_{i=1}^n a_i - \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j x_i^T x_j \label{eq:dual_no_zero}
\end{equation}

Note that \autoref{eq:dual_no_zero} no longer includes neither $w$, nor $b$, so it can be safely expressed without the minimum. As such we finally get the dual form of the SVM problem that we can solve:

\begin{equation}
\max\limits_{a: a_i \geq 0} \sum_{i=1}^n a_i - \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j x_i^T x_j \label{eq:dual_final}
\end{equation}

Subject to:

\begin{equation}
\sum_{i=1}^n a_i y_i = 0
\end{equation}

##### Recovering optimal bias parameter

The optimal value for $b$ must be one that pushes the separating hyperplane to sit between the furthest support vector in $w$'s direction and the closest support vector of the other class. In other words their functional margins $y_i(w^Tx_i+b)$ must be equal:

\begin{align}
\min\limits_{i:y_i=1} w^T x_i + b &= - (\max\limits_{i:y_i=-1} w^T x_i + b) \Rightarrow \\
b &= - \frac{1}{2}(\min\limits_{i:y_i=1} w^T x_i + \max\limits_{i:y_i=-1} w^T x_i)
\end{align}

#### Kernel Trick  {#sec:kernel}

As a linear classifier, a linearly separable dataset is usually required \ref{sec:linear}. But what we can do is perform a feature transform to a space where it is linearly separable.

<!-- TODO: image of distance from the origin example -->

\begin{theorem}
Mercer's theorem. Let $x \in \mathbb{R}^l$ and a mapping $\phi$:
\begin{equation}
x \mapsto \phi(x) \in H
\end{equation}
where $H$ is a Hilbert space. The inner product operation has an equivalent representation.
\begin{equation}
\langle\phi(x),\phi(z)\rangle = K(x,z)
\end{equation}
where $\langle\cdot,\cdot\rangle$ denotes the inner product operation in $H$ and $K(x,z)$ is a symmetric continuous function satisfying the following conditions (known as Mercer's conditions):
\begin{equation}
\int_C \int_C K(x,z)g(x)g(z)dx dz \geq 0
\end{equation}
for any $g(x), x \in C \subset \mathbb{R}^l$ such that:
\begin{equation}
\int_C g(x)^2 dx < + \infty
\end{equation}
where C is a compact (finite) subset of $\mathbb{R}^l$.
\cite{precog}
\end{theorem}

From Mercer's theorem we can assume that a mapping $\phi: R^d \rightarrow R^{d\prime}$ from $R^d$ to $R^{d\prime}$ has an equivalent kernel function $K$. We can now apply the mapping to the training dataset $X$ before training begins and get a dataset $X\prime$. Then run SVM to find a separating hyperplane on the new $X\prime$ dataset. We will now need to first apply the transform to any new data points, before we can make predictions on them.


Essentially we have replaced the inner product of the feature vectors with a kernel function $K(x_1, x_2) = \phi(x_1)^T\phi(x_2)$. With this in mind we can rewrite \autoref{eq:dual_final}:

\begin{equation}
\max\limits_{a: a_i \geq 0} \sum_{i=1}^n a_i - \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j K(x_i, x_j) \label{eq:dual_kernel}
\end{equation}

Subject to:

\begin{equation}
\sum_{i=1}^n a_i y_i = 0
\end{equation}

With:

\begin{align*}
b =& -\frac{1}{2}(\min\limits_{i:y_i=1} \sum_{a_j \neq 0} a_j y_j K(x_i, x_j) + \max\limits_{i:y_i=-1} \sum_{a_j \neq 0} a_j y_j K(x_i, x_j)) \\
w =& \sum_{i=1}^n a_i y_i \phi(x_i)
\end{align*}

There are many kernels that are used with SVM:

|  Name                                | Kernel                                        |
|--------------------------------------|-----------------------------------------------|
| Linear Kernel                        |  $\langle x,x\prime \rangle$                  |
| Polynomial Kernel                    |  $(\gamma\langle x,x\prime \rangle + r)^d$    |
| Exponential Kernel                   |  $exp(-\gamma||x-x\prime||^2)$                |
| Radial Basis Function                |  $exp(-\gamma ||x — x\prime||^2)$             |

Table: Kernels used with SVM

Where $d$ is the degree of the polynomial and $r$, $\gamma$ are bias parameters.

#### Regularisation

For some non linearly separable datasets, the non separability might be because of a few outliers in the dataset. In this case using a feature transform might not be the best way to deal with the dataset. What we can do instead is allow a few samples to be misclassified by adding slack variables. The primal form with slack variables is defined as such:

\begin{equation}
\min\limits_{w,b,\xi} \nicefrac{1}{2}||W||^2 + C \sum_{i=1}^n \xi_i
\end{equation}

Subject to:

\begin{align}
y_i(w^T x_i + b) \geq 1 - \xi_i \; &\forall i \in [1,n] \\
\xi_i \geq 0 \; &\forall i \in [1,n] 
\end{align}

Where $\xi_i$s are the slack variables, and $C$ is the misclassification "cost".

Following the same methodology as before it is trivial to show that the problem function remains convex and the constraints linear.

The dual form of the regularized SVM problem is defined as:

\begin{align}
\max\limits_{a,r \geq 0} \min\limits{w,b,\xi} & L(w,b,\xi, a, r) \\
\max\limits_{a,r \geq 0} \min\limits{w,b,\xi} & \nicefrac{1}{2} ||W||^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n a_i (y_i(w^T x_i + b) - 1 + \xi_i) - \sum_{i=1}^n r_i \xi_i \label{eq:dual_reg}
\end{align}

Subject to:

\begin{align}
y_i(w^T x_i + b) \geq 1 - \xi_i \; &\forall i \in [1,n] \\
\xi_i \geq 0 \; &\forall i \in [1,n] 
\end{align}

From \autoref{eq:KKT} again, we know the partial derivatives of the generalized Lagrangian will be equal to zero. Taking the partial derivative of \autoref{eq:dual_reg} in respect to $w_j$:

\begin{align}
\frac{\partial L(w,b,\xi,a,r)}{\partial w_j} &= 0 \Rightarrow \\
\frac{\partial \nicefrac{1}{2} ||W||^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n a_i (y_i(w^T x_i + b) - 1 + \xi_i) - \sum_{i=1}^n r_i \xi_i}{\partial w_j} &= 0 \Rightarrow \\
w_j - \sum_{i=1}^n a_i y_i x_{i,j} &= 0 \Rightarrow \\
w &= \sum_{i=1}^n a_i y_i x_{i,j}
\end{align}

So $w$ remains unchanged.

Taking the partial derivative of \autoref{eq:dual_reg} with respect to $b$:

\begin{align}
\frac{\partial L(w,b,\xi,a,r)}{\partial b} &= 0 \Rightarrow \\
\frac{\partial \nicefrac{1}{2} ||W||^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n a_i (y_i(w^T x_i + b) - 1 + \xi_i) - \sum_{i=1}^n r_i \xi_i}{\partial b} &= -\sum_{i=1}^n a_i y_i \Rightarrow \\
\sum_{i=1}^n a_i y_i &= 0
\end{align}

Again this remains unchanged.

Taking the partial derivative of \autoref{eq:dual_reg} in respect to $\xi_i$:

\begin{align}
\frac{\partial L(w,b,\xi,a,r)}{\partial \xi_i} &= 0 \Rightarrow \\
\frac{\partial \nicefrac{1}{2} ||W||^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n a_i (y_i(w^T x_i + b) - 1 + \xi_i) - \sum_{i=1}^n r_i \xi_i}{\partial \xi_i} &= -\sum_{i=1}^n a_i y_i \Rightarrow \\
C - a_i - r_i &= 0 \Rightarrow \\
C &= a_i + r_i
\end{align}


Now by substituting the above results into \autoref{eq:dual_reg} the dual regularized form can be rewritten as:

\begin{equation}
\max\limits_{a,r \geq 0} \min\limits{w,b,\xi} \sum_{i=1}^n a_i - \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j x_i^T x_j + \sum_{i=1}^n(C- a_i - r_i) \xi_i
\end{equation}

Subject to:

\begin{align*}
&\sum_{i=1}^n a_i y_i = 0 \\
&C = a_i + r_i \; \forall i \in [1,n]
\end{align*}

We can now note that $r$ does not appear in the problem function and that we can always choose $r_i \geq 0$ such that $C=a_i + r_i$ as long as $a_i  \leq C$. We can also note that $w$, $b$ nor $\xi$ appear. Rewriting the regularized dual form again yields:


\begin{equation}
\max\limits_{a} \sum_{i=1}^n a_i - \nicefrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j x_i^T x_j \label{eq:dual_reg_final}
\end{equation}

Subject to:

\begin{align}
&\sum_{i=1}^n a_i y_i = 0 \label{eq:lincon} \\
& 0 \leq a_i \leq C \; \forall i \in [1,n]
\end{align}

::: {.callout-note}
The kernel trick from \numnameref{sec:kernel}, still applies.
:::

## Sequential Minimization Optimization {#sec:smo}

The dual form we have derived in \autoref{eq:dual_final}, as well as the kernelized form in \autoref{eq:dual_kernel} can be solved using standard quadratic programming (QP) solvers. Using those can be very performance and memory intensive. For that reason many techniques where developed to speed up SVM. One of the more successful ones was Sequential Minimization Optimization (SMO) which takes the relatively large QP problem of SVM, breaks it into many smaller ones, and solves them analytically [@smo]. For each optimization step due to the linear constraint in \autoref{eq:lincon}, two Lagrange multipliers are jointly optimized at a time.

More specifically a high level overview of how the SMO algorithm works is as follows:

1. Pick one Lagrange multiplier to $a_1$ optimize, using the First Choice Heuristic.

2. Pick a second Lagrange multiplier $a_2$ to optimize, using the Second Choice Heuristic.

3. Calculate the prediction error $E_i = \text{Output of SVM on point i} - y_i$ for the first multiplier.

4. Using the prediction errors, perform what is essentially coordinate descend [@coordinate, page 11], to move both multipliers closer to their optimal value.

5. This is repeated until all multipliers satisfy the KKT conditions, within a small margin $\epsilon$, the original paper [@smo, page 48, Loose KKT Conditions] recommends a value in the range of $10^{-2}$ to $10^{-3}$ for the margin.

The First Choice Heuristic attempts to find a Lagrange multiplier that violates the KKT conditions. To speed up the training multipliers that are bounded^[bounded multipliers: when $a_i \neq C,\; a_i \neq 0$] are ignored --except if a full pass over the training set has not found a violating multiplier. In this case a full pass over all multipliers is done to find violating ones.

The Second Choice Heuristic attempts to find a second Lagrange multiplier, $a_2$, one that maximises the absolute value of the prediction error on the samples $i_1$ and $i_2$ given an already decided $i_1$.

<!-- TODO: expand upon a_2 new = a old - y2 (E1 - E2) blah blah -->
